{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Dataframe API\n",
    "**Andrey Titov**\n",
    "Senior Spark Engineer @ NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Устройство Spark Dataframe API\n",
    "+ Чтение данных из источника\n",
    "+ Работа с данными\n",
    "  - Базовый SQL\n",
    "  - NA функции\n",
    "  - Группировки\n",
    "  - Запись данных\n",
    "  - Соединения\n",
    "  - Оконные функции\n",
    "  - Функции pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Файлы с данными\n",
    "json_file = 'cities.json'\n",
    "output_parquet_agg = \"tmp/agg0.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Устройство Spark Dataframe API\n",
    "\n",
    "**Dataframe:**\n",
    "+ структурированная колоночная структура данных\n",
    "+ может быть создана на основе:\n",
    "  - локальной коллекции\n",
    "  - файла (файлов)\n",
    "  - базы данных\n",
    "+ в python работает значительно быстрее, чем RDD\n",
    "+ под капотом использует RDD\n",
    "+ позволяет выполнять произвольные SQL операции с данными\n",
    "+ аналогично RDD являются ленивыми и неизменяеыми\n",
    "\n",
    "## Из чего состоит Dataframe\n",
    "+ схема [pyspsark.sql.StructType](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType)\n",
    "+ колонки [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column)\n",
    "+ данные [pyspark.sql.Row](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select(\n",
    "    col(\"continent\"), col(\"country\")\n",
    ").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтение данных из источника\n",
    "Основной метод чтения любых источников\n",
    "\n",
    "```df = spark.read.format(datasource_type).option(datasource_options).load(object_name)```\n",
    "\n",
    "+ ```datasource_type``` - тип источника (\"parquet\", \"json\", \"cassandra\") и т. д.\n",
    "+ ```datasource_options``` - опции для работы с источником (логины, пароли, адреса для подключения и т. д.)\n",
    "+ ```object_name``` - имя таблицы/файла/топика/индекса\n",
    "\n",
    "[DataframeReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader):\n",
    "+ по умолчанию выводит схему данных\n",
    "+ является трансформацией (ленивый)\n",
    "+ возвращает [Dataframe](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "\n",
    "### Список (неполный) поддерживаемых источников данных\n",
    "+ Файлы:\n",
    "  - json\n",
    "  - text\n",
    "  - csv\n",
    "  - orc\n",
    "  - parquet\n",
    "+ Базы данных\n",
    "  - elasticsearch\n",
    "  - cassandra\n",
    "  - jdbc\n",
    "  - hive\n",
    "+ Брокеры сообщений\n",
    "  - kafka\n",
    "  \n",
    "\n",
    "**Библиотеки для работы с источниками должны быть доступны в JAVA CLASSPATH на драйвере и воркерах!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(json_file)\n",
    "\n",
    "df.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем ненужные колонки\n",
    "df.drop(\"_corrupt_record\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем дубликаты\n",
    "df \\\n",
    "    .drop(\"_corrupt_record\") \\\n",
    "    .distinct() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем пустые строки. Параметр \"all\" означает, что будут удалены только те строки, в которых ВСЕ элементы null\n",
    "df \\\n",
    "    .drop(\"_corrupt_record\") \\\n",
    "    .distinct() \\\n",
    "    .na.drop(\"all\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем пустые значения\n",
    "\n",
    "df \\\n",
    "    .drop(\"_corrupt_record\") \\\n",
    "    .distinct() \\\n",
    "    .na.drop(\"all\") \\\n",
    "    .na.fill( {'continent': 'Earth', 'population': 0 } ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример выборки нескольких колонок\n",
    "\n",
    "clean_data = df \\\n",
    "    .drop(\"_corrupt_record\") \\\n",
    "    .distinct() \\\n",
    "    .na.drop(\"all\") \\\n",
    "    .na.fill( {'continent': 'Earth', 'population': 0 } ) \\\n",
    "    .select(\"continent\", \"country\", \"name\", \"population\") \\\n",
    "\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Строим базовую группировку\n",
    "\n",
    "clean_data.groupBy('continent').count().show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод count можно спрятать внутри agg()\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "agg = clean_data.groupBy('continent').agg(count(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы колонки имели правильное имя, следует использовать метод alias()\n",
    "\n",
    "agg = clean_data.groupBy('continent').agg(count(\"*\").alias(\"count\"))\n",
    "agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим в группировку сумму населения на каждом континенте\n",
    "\n",
    "from pyspark.sql.functions import count, sum\n",
    "\n",
    "agg = \\\n",
    "    clean_data \\\n",
    "    .groupBy('continent') \\\n",
    "    .agg(count(\"*\").alias(\"city_count\"), sum('population').alias(\"population_sum\"))\n",
    "\n",
    "\n",
    "agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запись данных\n",
    "Основной метод записи в любые системы\n",
    "\n",
    "```df.write.format(datasource_type).options(datasource_options).mode(savemode).save(object_name)```\n",
    "\n",
    "+ ```datasource_type``` - тип источника (\"parquet\", \"json\", \"cassandra\") и т. д.\n",
    "+ ```datasource_options``` - опции для работы с источником (логины, пароли, адреса для подключения и т. д.)\n",
    "+ ```savemode``` - режим записи данных (добавление, перезапись и т. д.)\n",
    "+ ```object_name``` - имя таблицы/файла/топика/индекса\n",
    "\n",
    "[DataFrameWriter](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter):\n",
    "+ метод ```save``` является действием\n",
    "+ позволяет работать с партиционированными данными (parquet, orc)\n",
    "+ не всегда валидирует схему и формат данных\n",
    "\n",
    "\n",
    "### Список (неполный) поддерживаемых источников данных\n",
    "+ Файлы:\n",
    "  - json\n",
    "  - text\n",
    "  - csv\n",
    "  - orc\n",
    "  - parquet\n",
    "+ Базы данных\n",
    "  - elasticsearch\n",
    "  - cassandra\n",
    "  - jdbc\n",
    "  - hive\n",
    "+ Брокеры сообщений\n",
    "  - kafka\n",
    "  \n",
    "\n",
    "**Библиотеки для работы с источниками должны быть доступны в JAVA CLASSPATH на драйвере и воркерах!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним данные в parquet, предварительно отфильтровав данные\n",
    "\n",
    "condition = col(\"continent\") != \"Earth\"\n",
    "\n",
    "agg \\\n",
    "    .filter(condition) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_parquet_agg)\n",
    "\n",
    "print(\"Ok! Data is written to {}\".format(output_parquet_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P.S.\n",
    "# Когда мы делаем .filter в DataFrame API, мы передаем условие типа pyspark.sql.column.Column.\n",
    "print(type(condition))\n",
    "\n",
    "# когда раньше мы использовали лямбда функции в RDD, мы передавали лямбда функцию:\n",
    "condition_old = lambda x: x != \"Earth\"\n",
    "print(type(condition_old))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соединения\n",
    "\n",
    "Join'ы позволяют соединять два DF в один по заданным условиям.\n",
    "\n",
    "По типу условия join'ы делятся на:\n",
    "+ equ-join - соединение по равенству одного или более ключей\n",
    "+ non-equ join - соединение по условию, отличному от равенства одного или более ключей\n",
    "\n",
    "По методу соединения join'ы бывают:\n",
    "![Joins](http://kirillpavlov.com/images/join-types.png)\n",
    "[Источник](http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/)\n",
    "\n",
    "При выполнении join Spark автоматически выбирает один [из доступных алгоритмов](https://youtu.be/fp53QhSfQcI) соединения и не всегда делает это оптимально, часто применяя cross join. Поэтому, в последних версиях Spark метод ```join()``` приведет к ошибке, если под капотом он будет использовать cross join. Отключить эту проверку можно с помощью опции ```--conf spark.sql.crossJoin.enabled=true```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для демонстрации работы join используем подгтовленные данные\n",
    "clean_data.printSchema()\n",
    "\n",
    "agg = spark.read.parquet(output_parquet_agg)\n",
    "agg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Самый простой join - inner join по равенству одной колонки\n",
    "joined = clean_data.join(agg, 'continent', 'inner')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join по равенству двух колонок. Поскольку двух одинаковых колонок у нас нет, мы создадим их из константы\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "left = clean_data.withColumn(\"x\", lit(\"x\"))\n",
    "right = agg.withColumn(\"x\", lit(\"x\"))\n",
    "\n",
    "joined = left.join(right, ['continent', 'x'], 'inner')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-equ left join\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "left = clean_data.withColumn(\"city_count_max\", lit(2)).withColumnRenamed(\"continent\", \"continent_left\")\n",
    "right = agg.withColumnRenamed(\"continent\", \"continent_right\")\n",
    "\n",
    "join_condition = (col(\"continent_left\") == col(\"continent_right\")) & (col(\"city_count\") < col(\"city_count_max\"))\n",
    "\n",
    "joined = left.join(right, join_condition, 'left')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-equ right join\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "left = clean_data.withColumn(\"city_count_max\", lit(2)).withColumnRenamed(\"continent\", \"continent_left\")\n",
    "right = agg.withColumnRenamed(\"continent\", \"continent_right\")\n",
    "\n",
    "join_condition = (col(\"continent_left\") == col(\"continent_right\")) & (col(\"city_count\") < col(\"city_count_max\"))\n",
    "\n",
    "joined = left.join(right, join_condition, 'right')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross join\n",
    "clean_data.crossJoin(agg).show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Один из самых простов вариантов ускорить работу join - сделать broadcast.\n",
    "# При этом DF будет целиком склонирован на каждый воркер, что минимизирует shuffle во время выполнения join'а\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined = clean_data.join(broadcast(agg), 'continent', 'inner')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оконные функции\n",
    "\n",
    "Оконные функции позволяют делать функции над \"окнами\" (кто бы мог подумать) данных\n",
    "\n",
    "Окно создается из класса [pyspark.sql.Window](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window) с указанием полей, определяющих границы окон и полей, определяющих порядок сортировки внутри окна:\n",
    "\n",
    "```window = Window.partitionBy(\"a\", \"b\").orderBy(\"a\")```\n",
    "\n",
    "Применяя окна, можно использовать такие полезные функции из [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions), как ```lag()``` и ```lead()```, а также эффективно работать с данными time-series, вычисляя такие параметры, как, например, среднее значение заданного поля за 3-х часовой интервал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В нашем случае, используя оконные функции, мы можем построить DF из предыдущих примеров c join, \n",
    "# но без использования соединения\n",
    "\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "window = Window.partitionBy(\"continent\")\n",
    "\n",
    "agg = clean_data \\\n",
    "    .withColumn(\"city_count\", F.count(\"*\").over(window)) \\\n",
    "    .withColumn(\"population_sum\", F.sum(\"population\").over(window)) \\\n",
    "\n",
    "agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции pyspark.sql.functions\n",
    "\n",
    "Spark обладает достаточно большим набором встроенных функций, доступных в [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions), поэтому перед тем, как писать свою UDF, стоит обязательно поискать нужную функцию в данном пакете.\n",
    "\n",
    "К тому же, все функции Spark принимают на вход и возвращают [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column), а это значит, что вы можете совмещать функции вместе\n",
    "\n",
    "**Также важно помнить, что функции и колонки в Spark могут быть созданы без привязки к конкретным данным и DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "avg_pop = \\\n",
    "    to_json(\n",
    "        struct(\n",
    "            (col(\"population_sum\") / col(\"city_count\")).alias(\"value\")\n",
    "        )\n",
    "    ).alias(\"avg_pop\")\n",
    "\n",
    "agg.select(col(\"*\"), avg_pop).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Большим преимуществом Spark по сравнению с большинством SQL ориентированных БД является наличие\n",
    "# встроенных функций работы со списками, словарями и структурами данных\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "all_in_one = agg.select(struct(*agg.columns).alias(\"allinone\"))\n",
    "\n",
    "all_in_one.printSchema()\n",
    "all_in_one.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Например, можно создавать массивы и объединять их\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "arrays = \\\n",
    "    spark.range(0,1) \\\n",
    "    .withColumn(\"a\", array(lit(1), lit(2), lit(3))) \\\n",
    "    .withColumn(\"b\", array(lit(4),lit(5),lit(6))) \\\n",
    "    .select(array_union(col(\"a\"), col(\"b\")).alias(\"c\"))\n",
    "\n",
    "\n",
    "arrays.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, в разделе [SQL, Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html) присутствует еще более широкий список функций, доступных в Spark. Некоторые из них отсутствуют в [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)! \n",
    "\n",
    "Эти функции нельзя использовать как обычные методы над [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column), однако вы можете использовать метод ```expr()``` для этого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В данном примере мы используем Java функцию с помощью функции java_method\n",
    "# Запомните этот пример и используйте всегда, когда вам не хватает какой-либо функции в pyspark, \n",
    "# доступной в Java, ведь, используя такой подход, вы не снижаете производительность вашей программы за счет\n",
    "# передачи данных между Python и JVM приложением Spark, и при этом вам не нужно уметь писать код на Java/Scala :)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.range(0,1).withColumn(\"a\", expr(\"java_method('java.util.UUID', 'randomUUID')\")).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "**Dataframe API**:\n",
    "+ мощный инструмент для работы с данными\n",
    "+ в отличие от RDD, Dataframe API устроен так, что все вычисления происходят в JVM\n",
    "+ обладает единым API для работы с различными источниками данных\n",
    "+ имеет большой набор встроенных функций работы с данными\n",
    "+ имеет возможность использовать в pyspark функции, доступные в Java\n",
    "\n",
    "# Спасибо!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
