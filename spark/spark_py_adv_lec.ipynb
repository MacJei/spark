{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оптимизация Spark вычислений\n",
    "**Andrey Titov**\n",
    "Senior Spark Engineer @ NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ настройка spark-submit\n",
    "+ создание SparkSession\n",
    "+ использование repartition и coalesce\n",
    "+ schema inference\n",
    "+ column projection\n",
    "+ partition pruning\n",
    "+ predicate pushdown\n",
    "+ оптимизация join'ов\n",
    "+ fair scheduler\n",
    "+ workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Файлы с данными\n",
    "json_file = 'nbagames.json'\n",
    "output_parquet_agg = \"tmp/nbagames.parquet\"\n",
    "output_test_file = \"tmp/test.parquet\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TEST_PARQUET_FILE\"] = output_test_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка spark-submit\n",
    "Любое приложение, использующее Spark, запускается с помощью spark-submit.\n",
    "\n",
    "Утилита spark-submit позволяет:\n",
    "+ выбрать мастер сервис (local, YARN, local, spark, Mesos)\n",
    "+ выбрать режим запуска (client, cluster)\n",
    "+ указать количество CPU и RAM у worker'ов\n",
    "+ указать количество worker'ов\n",
    "+ указать количество CPU и RAM у драйвера\n",
    "+ настроить логирование\n",
    "+ подключить зависимости\n",
    "+ добавить переменные окружения\n",
    "+ подгрузить файлы\n",
    "+ настроить параметры datasource'ов\n",
    "+ настроить [другие параметры](https://spark.apache.org/docs/latest/configuration.html) приложения\n",
    "\n",
    "**Скрипт запуска этого ноутбука**\n",
    "```bash\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip=0.0.0.0 --NotebookApp.token= --port=8088'\n",
    "export PYSPARK_PYTHON=python3\n",
    "SPARK_VERSION=2.4.0\n",
    "\n",
    "pyspark2 \\\n",
    "    --conf spark.ui.port=8089 \\\n",
    "    --conf \"spark.driver.extraJavaOptions=-Dlog4j.configuration=file:$PWD/conf/log4j.properties\" \\\n",
    "    --conf \"spark.executor.extraJavaOptions=-Dlog4j.configuration=file:$PWD/conf/log4j.properties\" \\\n",
    "    --conf spark.sql.execution.arrow.enabled=true \\\n",
    "    --conf spark.sql.crossJoin.enabled=true \\\n",
    "    --master yarn \\\n",
    "    --deploy-mode client \\\n",
    "    --num-executors 1 \\\n",
    "    --executor-cores 1 \\\n",
    "    --jars /home/bigdatateam_instructor/atitov/spark-examples/lib/udf-funcs_2.11-0.1.jar \\\n",
    "    --packages com.datastax.spark:spark-cassandra-connector_2.11:$SPARK_VERSION,org.apache.spark:spark-sql-kafka-0-10_2.11:$SPARK_VERSION\n",
    "```\n",
    "\n",
    "JAVA зависимости следует искать на сайте [mvnrepository.com](https://mvnrepository.com)\n",
    "\n",
    "Посмотреть все параметры spark-submit можно через ```spark-submit --help```\n",
    "\n",
    "Остальные параметры берутся из ```$SPARK_CONF_DIR/spark-defaults.conf```, а переменные окружения из ```$SPARK_CONF_DIR/spark-env.sh```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание SparkSession\n",
    "+ для использования Spark API в приложении должен быть создан ```pyspark.sql.SparkSession``` (для RDD - ```pyspark.SparkContext```)\n",
    "+ в pyspark и spark-shell данные классы создаются автоматически\n",
    "\n",
    "**Пример создания ```SparkSession```**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SimpleApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .option(\"foo\", \"bar\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "**Избегайте установки параметров Spark внутри приложения!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule of thumb\n",
    "+ оптимальное число ядер на worker 5-8\n",
    "+ оптимальное количество памяти на worker 8-24 GB\n",
    "+ отключайте уровень логирования INFO\n",
    "+ используйте G1GC\n",
    "+ при сайзинге соблюдайте баланс между CPU, RAM, Network\n",
    "+ не избегайте новых версий Spark только потому, что они отсутствуют в коробке Hortonworks/Cloudera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование repartition и coalesce\n",
    "+ для изменения количества партиций в Spark реализовано две функции: ```repartition()``` и ```coalesce()```\n",
    "+ ```coalesce()``` позволяет уменьшить количество партиций и при этом не вызывает shuffle\n",
    "+ ```repartition()``` позволяет уменьшать и увеличивать количество партиций и использует shuffle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример использования coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Записывать много маленьких партиций - плохая идея\n",
    "file_with_many_partitions = spark.range(0,1000,1,100)\n",
    "print(file_with_many_partitions.rdd.getNumPartitions())\n",
    "file_with_many_partitions.write.mode(\"overwrite\").parquet(output_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $TEST_PARQUET_FILE | grep parquet | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Использование coalesce() позволяет сократить количество партиций и при этом не передает данные по сети\n",
    "file_with_many_partitions = spark.range(0,1000,1,100)\n",
    "file_with_many_partitions.coalesce(4).write.mode(\"overwrite\").parquet(output_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $TEST_PARQUET_FILE | grep parquet | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример использования repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим датасет с перекосом данных\n",
    "from pyspark.sql.functions import lit, col, when, rand\n",
    "left = spark.range(0,1000, 1, 4) \\\n",
    "    .withColumn(\"a\", when(col(\"id\") < 900, lit(\"one\")).otherwise(lit(\"two\"))) \\\n",
    "    .withColumn(\"r1\", rand() * 1000)\n",
    "\n",
    "right = spark.range(0,10, 1, 4) \\\n",
    "    .withColumn(\"b\", when(col(\"id\") < 9, lit(\"one\")).otherwise(\"two\")) \\\n",
    "    .withColumn(\"r2\", rand() * 1000)\n",
    "    \n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "skewed = left.join(right, col(\"a\") == col(\"b\"))\n",
    "\n",
    "print([len(x) for x in skewed.rdd.glom().collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition() убирает перекос данных между воркерами, \n",
    "# однако распределение данных по партияим будет произвольным\n",
    "repartitioned = skewed.repartition(100)\n",
    "print([len(x) for x in repartitioned.rdd.glom().collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно делать repartition по значению колонки или колонок. Поле \"a\" - не самый лучший кандидат для этого\n",
    "repartitioned = skewed.repartition(100, col(\"a\"))\n",
    "print([len(x) for x in repartitioned.rdd.glom().collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Правильнее использовать колонки \"r1\"и \"r2\"\n",
    "repartitioned = skewed.repartition(100, col(\"r1\"), col(\"r2\"))\n",
    "print([len(x) for x in repartitioned.rdd.glom().collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema inference\n",
    "При работе с большими JSON файлами ```spark.read``` отрабатывает не мгновенно. Это происходит из-за того, что Spark вынужден \"выводить\" схему данных из файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "df = spark.read.json(json_file)\n",
    "end = time()\n",
    "print(end - start)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схему можно указать вручную. Для этого есть два варианта:\n",
    "+ указать схему в формате DDL String \n",
    "+ передать класс ```pyspark.sql.types.StructType```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDL String\n",
    "from time import time\n",
    "start = time()\n",
    "schema = \"\"\"`_id` STRUCT<`$oid`: STRING>, teams ARRAY<STRUCT<abbreviation: STRING, city: STRING>>\"\"\"\n",
    "df = spark.read.schema(schema).json(json_file)\n",
    "end = time()\n",
    "print(end - start)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.types\n",
    "from time import time\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "start = time()\n",
    "schema = \\\n",
    "    StructType(\n",
    "    [\n",
    "        StructField(\n",
    "            \"_id\", \n",
    "            StructType([StructField(\"$oid\", StringType())])\n",
    "        ),\n",
    "        StructField(\n",
    "            \"teams\",\n",
    "            ArrayType(\n",
    "                StructType(\n",
    "                    [\n",
    "                    StructField(\"abbreviation\", StringType()),\n",
    "                    StructField(\"city\", StringType())\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    )\n",
    "\n",
    "\n",
    "df = spark.read.schema(schema).json(json_file)\n",
    "end = time()\n",
    "print(end - start)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используемые схемы также применимы для некоторых функций ```pyspark.sql.functions```, например:\n",
    "+ ```from_json()```\n",
    "+ ```cast()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column projection\n",
    "Данный механизм позволяет избегать вычитывания ненужных колонок при работе с источниками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запишем данные в parquet\n",
    "spark.read.json(json_file).drop(\"_corrupt_record\").write.mode(\"overwrite\").parquet(output_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.read.parquet(output_test_file)\n",
    "start = time()\n",
    "some_columns = df.select(col(\"_id.$oid\"), col(\"date.$date\"))\n",
    "\n",
    "some_columns.cache()\n",
    "some_columns.count()\n",
    "\n",
    "some_columns.explain(True)\n",
    "\n",
    "end = time()\n",
    "\n",
    "some_columns.unpersist()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.read.parquet(output_test_file)\n",
    "start = time()\n",
    "some_columns = df\n",
    "some_columns.cache()\n",
    "some_columns.count()\n",
    "some_columns.explain(True)\n",
    "\n",
    "end = time()\n",
    "\n",
    "some_columns.unpersist()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition pruning\n",
    "Данный механизм позволяет избежать чтения ненужных партиций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запишем данные в parquet\n",
    "from pyspark.sql.functions import explode, col\n",
    "spark.read.json(json_file) \\\n",
    "    .select(explode(col(\"teams\")).alias(\"team\")) \\\n",
    "    .select(col(\"team.*\")) \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"city\", \"abbreviation\") \\\n",
    "    .parquet(output_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(output_test_file)\n",
    "\n",
    "agg = df.filter((col(\"city\") == \"Los Angeles\") & col(\"abbreviation\").isin(\"LAL\", \"LAC\"))\n",
    "agg.show()\n",
    "agg.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicate pushdown\n",
    "Данный механизм позволяет \"протолкнуть\" условия фильтрации данных на уровень datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запишем данные в parquet\n",
    "from pyspark.sql.functions import explode, col\n",
    "spark.read.json(json_file) \\\n",
    "    .select(explode(col(\"teams\")).alias(\"team\")) \\\n",
    "    .select(col(\"team.*\")) \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"city\", \"abbreviation\") \\\n",
    "    .parquet(output_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(output_test_file)\n",
    "\n",
    "agg = df.filter(col(\"won\") == 1)\n",
    "agg.show()\n",
    "agg.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация join\n",
    "При выполнении join таблиц важно следовать рекомендациям:\n",
    "+ фильтровать данные до join'а\n",
    "+ использовать equ join \n",
    "+ если можно путем увеличения количества данных применить equ join вместо non-equ join'а, то делать именно так\n",
    "+ всеми силами избегать cross-join'ов\n",
    "+ если правый DF помещается в памяти worker'а, использовать broadcast()\n",
    "\n",
    "### Как посмотреть какой вид join'а будет использоваться?\n",
    "```df.join(other_df).explain(True)```\n",
    "\n",
    "Виды join'ов (от быстрого к медленному):\n",
    "+ BroadcastHashJoin\n",
    "  - equ-join, using broadcast\n",
    "+ SortMergeJoin\n",
    "  - equ-join, sortable keys\n",
    "+ BroadcastNestedLoopJoin\n",
    "  - using broadcast\n",
    "+ CartesianProduct\n",
    "  - все остальные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "left = spark.range(0,100).withColumn(\"a\", lit(\"a\")).withColumnRenamed(\"id\", \"id_left\")\n",
    "right = spark.range(0,10).withColumn(\"b\", lit(\"b\")).withColumnRenamed(\"id\", \"id_right\")\n",
    "\n",
    "\n",
    "# BroadcastHashJoin\n",
    "left.join(broadcast(right), col(\"id_left\") == col(\"id_right\"), 'inner')\n",
    "\n",
    "# SortMergeJoin\n",
    "left.join(right, col(\"id_left\") == col(\"id_right\"), 'inner')\n",
    "\n",
    "# BroadcastNestedLoopJoin\n",
    "left.join(broadcast(right), col(\"id_left\") < col(\"id_right\"), 'inner')\n",
    "\n",
    "# CartesianProduct\n",
    "left.join(right, col(\"id_left\") < col(\"id_right\"), 'inner')\n",
    "\n",
    "# CartesianProduct\n",
    "left.crossJoin(right).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair Scheduler\n",
    "+ fair scheduler - режим работы Spark, позволяющий выполнять несколько stage параллельно\n",
    "+ по умолчанию выключен\n",
    "+ при построении высоконагруженных приложений его следует использовать\n",
    "\n",
    "### Включение планировщика\n",
    "```bash\n",
    "spark.scheduler.mode FAIR\n",
    "spark.scheduler.allocation.file \"/path/to/fairscheduler.xml\"\n",
    "```\n",
    "\n",
    "Пример fairscheduler.xml\n",
    "```xml\n",
    "<?xml version=\"1.0\"?>\n",
    "<allocations>\n",
    "  <pool name=\"pool0\">\n",
    "    <schedulingMode>FAIR</schedulingMode>\n",
    "    <weight>1</weight>\n",
    "    <minShare>2</minShare>\n",
    "  </pool>\n",
    "</allocations>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "df = spark.read.json(json_file)\n",
    "\n",
    "ui_url = sc._jsc.sc().uiWebUrl().get()\n",
    "\n",
    "webbrowser.open(ui_url + '/stages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Все стейджы выполняются последовательно\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from time import time\n",
    "\n",
    "for _ in range(20):\n",
    "    df.na.drop().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![без fair scheduler](https://github.com/tenkeiu8/spark-examples/blob/master/images/without_fair_scheduler.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стейджы выполняются параллельно в дефолтном пуле\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql.functions import col\n",
    "from time import time\n",
    "\n",
    "def process_df(df):\n",
    "    df.na.drop().count()\n",
    "\n",
    "with ThreadPoolExecutor(4) as executor:\n",
    "    future_tasks = []\n",
    "    for _ in range(20):\n",
    "        future_tasks.append(executor.submit(process_df, df))\n",
    "    \n",
    "    for future in as_completed(future_tasks):\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![with_fair_scheduler](https://github.com/tenkeiu8/spark-examples/blob/master/images/with_fair_scheduler.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стейджы выполняются параллельно в пуле pool0\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql.functions import col\n",
    "from time import time\n",
    "\n",
    "def process_df(spark, df):\n",
    "    spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool0\")\n",
    "    df.na.drop().count()\n",
    "\n",
    "with ThreadPoolExecutor(4) as executor:\n",
    "    future_tasks = []\n",
    "    for _ in range(20):\n",
    "        future_tasks.append(executor.submit(process_df, spark, df))\n",
    "    \n",
    "    for future in as_completed(future_tasks):\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fair_scheduler_with_pool](https://github.com/tenkeiu8/spark-examples/blob/master/images/fair_scheduler_with_pool.png?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
