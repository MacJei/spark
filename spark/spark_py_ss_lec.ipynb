{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming\n",
    "**Andrey Titov**\n",
    "Senior Spark Engineer @ NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "+ Rate stream creation\n",
    "+ Writing streaming data to console\n",
    "+ Writing data to kafka\n",
    "+ Persisting stream state using checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data locations\n",
    "json_file = 'cities.json'\n",
    "streaming_dir = 'tmp/streaming_dir'\n",
    "kafka_servers = 'localhost:9092'\n",
    "kafka_topic = 'test_topic'\n",
    "checkpoint_dir = \"tmp/chk/chk_1\"\n",
    "\n",
    "output_parquet_agg = \"tmp/agg0.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating rate >> console stream\n",
    "+ rate source generates random data\n",
    "+ console sink is used to print data to console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_sdf = spark.readStream.format(\"rate\").load()\n",
    "rate_sdf.printSchema()\n",
    "rate_sq = rate_sdf.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_sq.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating text >> console stream\n",
    "+ text source listens for new files in folder\n",
    "+ new files are detected by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sdf = spark.readStream.format(\"text\").option(\"path\", streaming_dir).load()\n",
    "file_sdf.printSchema()\n",
    "file_sq = file_sdf.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sq.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing file to kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = spark.read.format(\"text\").load(json_file)\n",
    "\n",
    "kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": kafka_servers\n",
    "}\n",
    "\n",
    "cities.write.format(\"kafka\").options(**kafka_params).option(\"topic\", kafka_topic).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating kafka >> console stream\n",
    "+ Kafka API is same for batch and streaming modes\n",
    "+ Dataframe contains the following columns:\n",
    " - `key`\n",
    " - `value`\n",
    " - `topic`\n",
    " - `partition`\n",
    " - `offset`\n",
    " - `timestamp`\n",
    " - `timestampType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf = spark.readStream.format(\"kafka\") \\\n",
    "                .options(**kafka_params) \\\n",
    "                .option(\"subscribe\", kafka_topic) \\\n",
    "                .option(\"startingOffsets\", \"earliest\") \\\n",
    "                .load()\n",
    "\n",
    "kafka_sdf.printSchema()\n",
    "\n",
    "kafka_sq = kafka_sdf.writeStream.format(\"console\").option(\"truncate\", \"true\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sq.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabled checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sdf = spark.readStream.format(\"text\").option(\"path\", streaming_dir).load()\n",
    "\n",
    "kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": kafka_servers,\n",
    "    \"topic\": kafka_topic\n",
    "}\n",
    "\n",
    "kafka_sq = file_sdf.writeStream.format(\"kafka\") \\\n",
    "                    .options(**kafka_params) \\\n",
    "                    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "                    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sq.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stream statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check is stream is active\n",
    "kafka_sq.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last batch information\n",
    "kafka_sq.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stream status\n",
    "kafka_sq.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from kafka and write it to console\n",
    "\n",
    "kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": kafka_servers,\n",
    "    \"subscribe\": kafka_topic,\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**kafka_params).load()\n",
    "console_sq = kafka_sdf.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_sq.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in spark.streams.active:\n",
    "    s.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
